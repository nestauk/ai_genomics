# A scientometric analysis of AI in genomics: Technical specification

India Kerle, Juan Mateos-Garcia, Jack Vines, Luca Bonavita and Samuel Doogan.

## Context

The Ada Lovelace Institute and the Nuffield Council for Bioethics are currently working on a project to map "AI genomics futures" and consider their societal and policy implications. Nesta will be contributing to the project through a scientometric analysis aimed at understanding key features of AI genomics Research and Development (R&D). Some potential areas of focus include:

1. What are the levels of R&D activity in the intersection of AI and genomics? How have they evolved over time?
2. What is the thematic composition of the field? What are "emergent themes" in AI and genomics?
3. What institutions are participating in AI genomics R&D? What is their geography? What is their character (e.g. public, private)?
4. What are the key differences between AI and genomics and the wider field of genomics in terms of application areas, participants and stakeholders, influence and impact?

The scientometric analysis will result in a report addressing questions such as those above that will provide an empirical context for the AI Genomics Futures project and inform subsequent activities such as a horizon-scanning exercise and stakeholder engagement activities.

### About this note

Section 2 describes the methodology for the project including data sources that we propose to use, how we will process them, enrich them and analyse them, and the research questions that these questions will help us address.

Section 3 sets out how we propose to deliver the project including its outputs, approach to project management, timelines and budget.

## Methodology

### High level methodological narrative

We propose to collect, enrich and analyse data about research, technology development and business activities in the intersection of AI and genomics. The enrichment activities seek to enhance the data with information about the geography and character of actors (e.g. researchers, inventors, entrepreneurs) participating in AI genomics R&D as well as the purpose and influence (e.g. citations, social media reach) of the R&D activities.

We will use Natural Language Processing (NPL) and machine learning methods to extract information from R&D activity descriptions. This will help us measure the composition of AI genomics R&D (e.g. key research themes and technological trajectores), identify emerging trends that might be of particular interest to the project and undertatnd their purpose (e.g. the disease areas they target, the products and services they seek to develop). This will also allow us to compare the specialisation profile of different countries / actors / types of actors (e.g. private sector vs academic researchers). Where possible we will benchmark the situation in AI genomics R&D vs. the wider field of genomics.

We will also explore options to integrate information about the composition and goals of AI genomics research across multiple data sources, develop experimental indicators about novelty, diversity and interdisciplinarity in AI genomics research and model outcomes of interest.

Next subsection we highlight key data streams for the project: the data we plan to collect, how we will identify AI and genomics research in it and how we will enrich it.

Having done this we outline potential methods to analyse these data.

### Data streams

#### Research data

There are three reasons to include published research in this project:

1. Research is likely to provide a timelier indicator of current and emerging trends in AI genomics than laggy patent and business activity data.
2. Earlier-stage, emerging trends that might be relevant in the future are more likely to be present in research data than other sources with information about the techniques that are beign applied today.
3. There are strong traditions of open publishing in genomics and AI research

We recognise that publication data would, on its own, provide a skewed view of the situation and evolution of AI genomics R&D (e.g. not distinguishing between theoretical developments and applications, and missing activities that business choose not to publish because of their commercial sensitivity). We will address these gaps by incorporating patent and business data into the project.

##### Collection

We will use [OpenAlex](https://www.openalex.org) as the core data source for research data in this project. OpenAlex is an open scientometric database developed to replace Microsoft Academic Graph, a database of academic publications that was recently discontinued. OpenAlex includes information about:

- Works (papers, books, datasets etc., [example](https://api.openalex.org/works/W2741809807))
- Authors (who create works, [example](https://api.openalex.org/authors/A2208157607))
- Venues (journals/repos that contains works, [example](https://api.openalex.org/venues/V1983995261))
- Institutions (organisations/institutions affiliated with a work, [example](https://openalex.org/I114027177))
- Concepts (tags works with topics [example](https://openalex.org/C2778407487))

OpenAlex is a new database but current discussions suggest that it is the best open dataset available, and that _it's coverage and accuracy_ is already comparable to established players such as Scopus or Dimensions.

The main ways of interacting with dataset are via the API, or through database snapshots. We currently envisage downloading the most recent snapshot (200GB) and storing it as collection of tables to work with.

We will identify papers related to AI and genomics through their `Concepts`, which are keywords assigned to the data based on a text analysis of their abstract. A preliminary exploration of the data has revealed around 400,000 papers with the `Genome` concept and `37,000` papers with the `Genome` and `Computer Science` concepts. One important task at the beginning of the project will be to generate a list of concepts that bound the field of AI & Genomics.

##### Processing

Having collected the OpenAlex dataset, we will enrich it in the following days:

- Add intitutional metadata: The institutional affiliations to OpenAlex paper include a `GRID` identifier which make it possible to enrich those institutions with metadata from [GRID](https://www.grid.ac/) (Global Research Indentifier Database), which includes an institution location at various levels of resolution and its character (whether it is an educational institution, a private sector organisation, a hospital, a government agency etc.)
- Add citation data: We will collect information about the citations in a paper and the citations that it has received using [Semantic Scholar](https://www.semanticscholar.org/), a platform that uses AI to advance scientific discovery. Semantic Scholar data can be accessed by querying its API with a document ID which we will obtain from OpenAlex. Citation data will be useful for proxying a paper's influence in the academic community, and to measure the bodies of knowledge that a paper builds on (e.g. does the paper mostly cite AI research, or research from the biological sciences, does it cite particular databases, does it cite papers from the Humanities suggesting that it takes into account ethical issues?).
- Add social influence engagement information (stretch goal): We could use [Crossref Event](https://www.crossref.org/services/event-data/) to measure level of public engagement with research publications in a variety of venues such as Twitter, Wikipedia, Reddit or WordPress. Crossref offers an [open API](https://www.eventdata.crossref.org/guide/) that can be queried with a paper's DOI (Digital Object Identifier) in order to obtain links to its social media engagements. We note that the actual content of an article's social engagements require additional data collection from its primary sources (e.g. querying the Twitter API), which we see as out of scope for this project. Here, we would simply generate an indicator with the number of social media engagements that an article has received, a proxy of public interest in it.

#### Patent data

Patents can helps us measure how particular AI genomics methods are being transformed into practical applications / technologies.

###### Collection

Having scoped a range of data sources (see table in the annex), we are currently considering the following pipeline for data collection and processing:

1. Use the USPTO (US Patent and Trademark Office ) [Artifical Intelligence Patent Dataset](https://www.uspto.gov/ip-policy/economic-research/research-datasets/artificial-intelligence-patent-dataset), a database of AI patents filed with the USPTO, to identify AI patents.
2. Query the USPTO API to collect additional information about those patents and label as "genomics" those with IPC (International Patent Classification) codes related to genomics or research in the biological sciences.
3. Optionally: Collect additional information about those patents from [Lens](https://www.lens.org/), a public resource about global patents and research knowledge which includes information about the institutions related to a patent. We might need to fuzzy-match these institutions with GRID in order to obtain their location and type of institution.

#### Business data

We are also interested in measuring business and investment activity in the intersection of AI and genomics.

##### Data collection

We will use [CrunchBase](https://www.crunchbase.com/), a widely used database with information about technology companies and startups and their investments that we have already licensed. More specifically:

1. We will identify AI genomics companies by analysing the keywords used to label companies and the text in their descriptions.
2. We will extract all relevant information about those companies including their description, location, year of incorporation and number of employees
3. We will identify "investment events" for those companies and their evolution, and the investors who participated in those events.

#### Other sources

We have also considered / have access to other data sources which might be relevant for the project but that we perceived as "stretch goal" rather than priorities. They include:

- Public funding data: We have relatively easy access to public funding data from the UK ([Gateway to Research](https://gtr.ukri.org/), EU ([CORDIS](https://cordis.europa.eu/)) and US ([NIH Reporter](https://reporter.nih.gov/)). It would be possible to analyse this data to measure levels of public support for AI genomics research and its topical focus but there are some fixed costs in ingesting and harmonise them and implementing robust definitons of AI and Genomics through a bottom-up analysis of grant descriptions / abstracts. One option would be to carry out a quantitative case study of the evolution and compositin of public funding for AI genomics in the UK using the GtR data.
- AI software and datasets: [Papers With Code](https://paperswithcode.com/) is a crowd / machine sourced platform that links AI papers to the GitHub repositories where they share their code, and to the datasets that we used. A preliminary analysis of the source revealed a limited number of papers / datasets that might be relevant for the project (~100 papers, 30 datasets mentioning genomics in their description).
- Open source software: There are ca. [1700 GitHub code repositories](https://github.com/topics/genomics) tagged with the `genomics` label, and [79,800 machine lerning repositories](https://github.com/topics/genomics). We could use the GitHub API to extract data about these repositories and analyse their content. Restrictions in the GitHub API rates mean that this would be a time-intensive task.
- [DrugBank](https://go.drugbank.com/) is a database with detailed information about drugs, their mechanisms, their interactions, and their protein targets. In principle it might be possible to analyse it in order to identify drugs / projects / analyses e.g. of pharmacogenomics (interactions between a drug and genome) that use machine learning methods but this would only capture one dimension of AI genomics and would require a significant investment in collecting, exploring and understanding the data.

### Analysis

Here we outline various strategies to analyse the data in order to address key questions for the project.

#### Entity extraction

We would like to measure the purpose / goal of AI + genomics R&D activities using a standardised set of terms. This would for example help us identify the disease areas that researchers focus on and the disease areas that they (perhaps) neglect as well as how this focus has evolved over time, and differences between countries, organisation types etc.

In order to do this, we will leverage a data pipeline being developed at Nesta which annotates text documents (e.g. abstracts) with their [DBPedia entities](https://www.dbpedia.org/resources/ontology/) and the level of confidence in the tag. Some of these entities are linked to Medical Subject Heading ([MeSH](https://www.nlm.nih.gov/mesh/meshhome.html)), an ontology developed by the US National Institute of Health to label biomedical research.

#### Thematic analysis

We can use other methods to characterise the composition of AI genomics sub-corpora including:

- Topic modelling of article / patent abstracts and company descriptions. This method yields the themes in a corpus (collections of terms that tend to appear in the same document) and estimates the relative importance (weight) of different topics in specific documents. We can use this to classify projects into their most important topics, measure overlap between topics and calculate more sophisticated indicators of research interdisciplinarity, diversity and novelty (see below for additional details).
- Document clustering: There are several vector representations quantifying the "meaning" of research articles and patents abstracts available from [Semantic Scholar](https://www.semanticscholar.org/) and [Google Patent Dataset](https://console.cloud.google.com/marketplace/product/google_patents_public_datasets/google-patents-public-data?project=hp-data-dumps). We could use these vector representations to cluster those documents into semantically similar groups that might capture specific research themes / application areas / techniques.

#### Additional analyses

Other analyses that we could undertake depending on time, resources and interest include:

- Influence analysis: In addition to using forward citations (citations received by a paper) to proxy its influence and impact, we could also analyse its backward citations (citations that it makes to other papers) to capture the bodies of knowledge it builds on e.g. their reliance on knwoledge from computer science versus biological sciences or even other disciplines. This could help us build measures of interdisciplinary crossover in genomics research.
- Advanced indicators: We could build advanced indicators such as:
  - A paper's interdisciplinarity based on the distribution of its thematic composition or influences (more concentrated thematic distributions will tend to be less interdisciplinarity)
  - Measures of novelty based on the rarity of the topics or subjects that the paper brings together, or the difficulties for clustering it within existing corpora of research.
  - Measures of thematic diversity in the research ideas that are explored by different research communities / countries or even the AI and genomics community writ large.
- Integrated thematic analysis: We could explore options to classify all documents (including e.g. articles, patents, companies, research grants) into the same set of categories / topics. A naive approach to do this where we combine all documents into a single corpus and cluster them is likely to pick up on differences in language across sources to separate documents back into their sources, so we might have to explore alternatives such as tagging documents with DBPedia entities (as discussed above) and clustering them on their frequencies, or performing clustering on a single corpus and then assigning documents from other corpora to their closest clusters in semantic space.

### Project delivery

#### Outputs

#### Approach to project management

#### Timelines and budget

### Annex

Below is a table summarising the various patent data sources we could use including ways of accessing the data, data sources covered in each dataset, pros and cons of each dataset and other notes.

| Dataset                                                                                                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                      | How to Access (Format, Restrictions)                                                                                                                                                                                                                                                                                                        | Temporal Coverage              | Data provenance and quality                                                                                                                                                                                                                                                                                                                                                                                                                                    | Update Cadence  | How to Store?         | How will we find AI / genomics papers?                                                                                                                                                                                                                                                                                                        | Pros                                                                                                                                                                                                 | Cons                                                                                                                                                                                                                       | Further Questions                                                                                                                                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- | --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Patent Lens](https://www.lens.org/)                                                                                                                          | Information on over 138 million patent records from different patent organisations.Theyâ€™re also mapping this patent data to other information including scholarly works, authors, applicants, owners and families. [Example patent query](https://docs.api.lens.org/examples-patent.html) / [response fields including sample patent record](https://docs.api.lens.org/response-patent.html) | Can access full corpus of Lens patent records via API or bulk download in json. you must pass a query. example query attached. In terms of restrictions, our use would be "commercial" according to [their terms of use](https://about.lens.org/lens-api-terms-of-use/). We would therefore have to pay 1000 dollars per annum for its use. | patent data from 1700s onwards | Patent and bibliographic data is obtained via partnerships with Microsoft Academic, CrossRef, ORCID, PubMed, Impactstory, CORE, European Patent Office, USPTO, IP Australia and World Intellectual Property Organisation. They also disambiguate and link patents, scholarly works, names and instituations in a graph. Potential data quality issues related to disambiguation efforts. [Release notes here.](https://about.lens.org/category/release-notes/) | bi-monthly      | s3?                   | Likely take a keyword approach to querying the API across text fields like abstract (the patent document abstract text), description (the description text of the patent document), patent title (title of the patent/invention) and claims.                                                                                                  | - Aggregated and in a single format so we won't have to spend time standardising data from EPO, UPSTO etc. - well documented, API appears easy to use                                                | - Aggregated so if we have any difficulty with this source, we will have difficulty across EPO, USPTO etc. sources - Not open source for commercial use                                                                    | Is there anything about our work that could help contribute to their open source mission? i.e. disambiguation, quality checking, translation of texts? Edward in Discovery spoke with Lens - apparently it "offers cheaper access to patent data, but not as user-friendly and they couldn't settle a deal within this FY" |
| [Global Patent Index](https://www.epo.org/searching-for-patents/technical/espacenet/gpi.html)                                                                 | Patent, bibliographic, legal event and full text data from the European Patent Office.                                                                                                                                                                                                                                                                                                       | Can access patents via a front end search and download search query results in .csv. We will need to purchase the data on a quarterly (250 euros), bi-annually (450 euros) or annual (755 euros) basis.                                                                                                                                     |                                | Data is obtained directly from EPO. [Release notes here.](https://www.epo.org/searching-for-patents/technical/espacenet/release-notes.html)                                                                                                                                                                                                                                                                                                                    | weekly          | s3?                   | Likely take a keyword approach, following [their search guidelines](https://documents.epo.org/projects/babylon/eponet.nsf/0/6648B645FE4F5C46C125839F00585141/$File/gpi_pocket_guide_V6_en.pdf) to query the database.                                                                                                                         |                                                                                                                                                                                                      | - not open source - no way to programmatically access the data via an API - only EPO patents - up front cost of setting up an account                                                                                      |                                                                                                                                                                                                                                                                                                                            |
| [Google Patent dataset](https://console.cloud.google.com/marketplace/product/google_patents_public_datasets/google-patents-public-data?project=hp-data-dumps) | Bibliographic information on more than 90 million patent publications from 17 countries and US full text, provided by IFI CLAIMS Patent Services. The Google Patents Research Data also includes english machine translations for titles and abstracts from Google Translate, similarity vectors, extracted top terms, similar documents and forward references.                             | Can access via BigQuery. The data is open, licensed under 4.0 creative commons.                                                                                                                                                                                                                                                             |                                | Data is obtained from [IFI CLAIMS Patent Services](https://www.ificlaims.com/start.htm) (which is itself an aggregator of patents accross 100+ sources). [A number of issues documented in their repo here.](https://github.com/google/patents-public-data/issues)                                                                                                                                                                                             | Quarterly       | Connect to its own db | We could either take a keyword approach or take advantage of a number of tools they have built on top of the patent data, including ["automated patent landscaping"](https://github.com/google/patents-public-data) that takes a semi-supervised approach to find patents related to a topic for which we can generate a seed set of patents. | - open source - Provides translations of non-english patents - Does a lot of the data science work for us by providing similarity vectors, a method to identify patent landscapes using seed patents | - learning curve to get comfortable using SQL and BigQuery - The text data does not look structured but TBD - updated less frequently than the other sources - More opaque data sources (who are IFI CLAIMS data sources?) | [Blog on both the patent and patent research data](https://cloud.google.com/blog/topics/public-datasets/google-patents-public-datasets-connecting-public-paid-and-private-patent-data). Could we use parts of this project rather than the dataset itself? i.e. use their word2vec model trained on patent data.           |
| [USPTO Artifical Intelligence Patent Dataset](https://www.uspto.gov/ip-policy/economic-research/research-datasets/artificial-intelligence-patent-dataset)     | U.S. patents issued between 1976 and 2020 and pre-grant publications published through 2020 that contain one or more of several AI technology components (including machine learning, natural language processing, computer vision, speech, knowledge processing, AI hardware, evolutionary computation, and planning and control).                                                          | Can download data directly in bulk in either .dta or .tsv format.                                                                                                                                                                                                                                                                           | 1976 - 2020                    | Data is obtained from USPTO. [Methodology of identifying AI patents (and associated methodological shortcomings) detailed here.](https://link.springer.com/article/10.1007/s10961-021-09900-2)                                                                                                                                                                                                                                                                 | Appears one off | s3?                   | Given the patents are already related to AI, we will just need to subset for genomics related patents.                                                                                                                                                                                                                                        | - patents already at least related to AI - open source                                                                                                                                               | - appears one off - only U.S. patents represented - given the approach to identifying AI papers will invariably not be 100% accurate, we will be "downstreaming" those errors                                              | Would be good to assess their [method of identifying AI patents in U.S. data](https://link.springer.com/article/10.1007/s10961-021-09900-2) and apply it to AI genomics patents across a number of patent data sources.                                                                                                    | This covers eight "AI component" technologies like NLP, AI hardware and ML but does it cover everything in relation to genomics? Do we want to "downstream" methodological errors? |
